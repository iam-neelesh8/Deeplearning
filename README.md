# #IMDB Movie Review Sentiment Analysis

Introduction:
Movies play an important role in peoples’ lives and are watched all over the world, so it is essential for people to spend this time watching movies they will enjoy. Whether watching for inspiration, relaxation, or building community, viewers want to select movies they expect to like. By looking at both reviews and ratings, prospective viewers can gather information to better make this choice. The authors attempt here to improve this decision-making by providing additional information in the form of an aggregate score of sentiment analysis on all reviews for each movie.

I validate our approach using reviews from the IMDb website, as they are available both as supervised and unsupervised datasets. Further, because of the unique structure and easy access via the IMDb website, movie reviews provide interesting opportunities for research into the design and implementation of deep learning models. As a part of the INFO-H 518 Deep Learning course at Indiana University, the authors intended to use the available data to improve their deep learning skills and come up with opportunities to investigate potential research avenues. The most interesting research direction we identified was to implement a model to generate new model reviews.

Problem description:
The initial problem we studied is sentiment analysis of individual reviews; that is, given the text of a movie review on IMDb (Internet Movie Database), classify the review as positive (rating >= 7 out of 10) or negative (rating <= 4 out of 10). Once I could classify individual reviews, I could then apply the model to collections of reviews, which lets us evaluate entire movies. This is useful because the distribution of reviews may change over time, whether because of various audiences watching the movie, or even changes in cultural expectations. Additionally, ratings may differ from reviews, since one can rate without reviewing, which may skew ratings depending on how passionate various groups of viewers are. For example, Shawshank Redemption had great reviews, but at times the ratings were not as strong, which may be explained by those who cared enough to review being more favorable. By aggregating the results of sentiment analysis, we hope to capture what fraction of engaged viewers liked the movie, and compare this to the ratings those viewers gave. To evaluate the idea that aggregate sentiment of reviews is a related but separate measure from ratings, I ran the model on 1 million IMdB reviews and plotted the results. Finally, I investigated interest in generating new reviews by implementing a model.

Methodology:
I began by implementing a simple baseline model for sentiment analysis. Among the many possible options, My first pass was to implement a fully connected network, listed as “ANN 1” in Table 1. This first pass performed surprisingly well, as it reached 88% validation set accuracy, despite having a relatively small training set. Perhaps this high accuracy is because words which commonly appeared in reviews, such as excellent, terrible, and entertaining, were used often enough that the embedding layer derived sufficient information from them.
Following the creation of the baseline, we started looking at recurrent neural networks and their variations, so our models could take advantage of the sequential nature of the reviews. Despite our high hopes, these models only matched the performance of the baseline, though they were able to do this with only 100 words per review, versus 2500 words for the baseline model. The LSTM model was the biggest surprise during this process, as it reached no more than 73% accuracy with many different combinations of hyperparameters, while the RNN and GRU were able to reach roughly 86% even on our first attempts.
Once our models were no longer showing improvements in performance with different architectures and hyperparameters, we selected our best model and focused our attention on aggregating our model results. We applied the same preprocessing on all available reviews for our dataset of movies, and then passed the resulting inputs into our best model and aggregated the results. We then joined the aggregated results to our movie dataset, resulting in the dataset shown in Figure 3.




	
	
